# ğŸï¸ğŸ’¡ Team XData -- Insurance Charges ML-based Predictions

### End-to-end repo for predicting **health insurance charges** with:
- **XGBoost** (gradient boosting) + **Bayesian hyperparameter tuning** (`BayesSearchCV`)
- **Random Forest** (bagging) + Bayesian tuning
- **Log-target modeling** with **Duan smearing** (global & subgroup)
- **Presentation-ready visuals** (parity plots, MAE by segment)
- **Sensitivity analysis** (what moves predictions & by how much)
- **Fairness slicing** (MAE by Sex / Region / Children)

---

## ğŸ”— Table of Contents
### â€¢ [Why this project?](#-why-this-project)  
### â€¢ [Modeling approach](#-modeling-approach)  
### â€¢ [Hyperparameter tuning](#-hyperparameter-tuning)  
### â€¢ [Log target & smearing](#-log-target--smearing)  
### â€¢ [Evaluation metrics (business-friendly)](#-evaluation-metrics-business-friendly)  
### â€¢ [Visualizations](#-visualizations)  
### â€¢ [Sensitivity analysis](#-sensitivity-analysis)  
### â€¢ [Fairness slices & recommendations](#-fairness-slices--recommendations)  
### â€¢ [Reproducibility & seeds](#-reproducibility--seeds)  
### â€¢ [Common errors & fixes](#-common-errors--fixes)  

---

## âœ¨ Why this project?
### Insurance pricing is **noisy** and **nonlinear** (thresholds at **BMIâ‰ˆ30**, smoker interactions).
### We need models that:
- **Learn nonlinearities & interactions** (XGBoost / RandomForest)
- **Tune efficiently** (Bayesian search over focused spaces)
- **Predict on $ scale with minimal bias** (log-model + **Duan smearing**)
- **Remain interpretable** (MAE, parity plots, sensitivity â€œwhat-ifâ€)

---

## ğŸ§  Modeling approach 

### XGBoost (boosting)

1. Stage-wise additive modeling: each tree fits the negative gradient (residuals) of the current ensemble.
2. Handles nonlinearities, interactions, and outliers robustly.
3. Regularized with gamma, reg_alpha, reg_lambda, plus subsampling to reduce overfitting.

### Random Forest (bagging)
1. Many decorrelated trees via bootstrap sampling and feature subsampling.
2. Strong variance reduction and stable baseline; often great MAPE on lower charges.
3. Linear & Elastic Net (optional)
3. With engineered interactions (e.g., SmokerÃ—BMI, SmokerÃ—Age) to reduce bias.

### Elastic Net adds L1/L2 for feature selection + stability under collinearity.

## ğŸ¯ Hyperparameter tuning

### Bayesian optimization (scikit-optimize BayesSearchCV)
- Smarter than exhaustive grid: balances exploration/exploitation via surrogate modeling (Gaussian Process or Tree-Parzen under the hood).
- Great for expensive model evaluations and mixed continuous/discrete spaces.

Recommended amount:
1. XGBoost: n_iter â‰ˆ 40â€“80
2. Random Forest: n_iter â‰ˆ 30â€“60 (RF is less sensitive than boosting)

### ğŸ“ˆ Log target & smearing
 **We train on log(charges) to stabilize variance. To go back to dollars without bias, apply Duan smearing:

- Train residuals (log scale): e_i = y_log_true - y_log_pred
- Smearing factor: S = mean(exp(e_i))
- Back-transform: Å·_$ = exp(Å·_log) * S
- We also support per-group smearing (e.g., by sex) to reduce subgroup bias if residual distributions differ.

### ğŸ“Š Evaluation metrics (business-friendly)
 **MAE â€” average absolute error in dollars; easy to interpret (â€œtypical error per policyâ€).
 **RMSE â€” penalizes big misses; good for risk control.
 **RÂ² â€” share of variance explained; high-level goodness of fit.
 **MAPE â€” relative error (%); highlights performance on low-charge cases.

For pricing ops, we typically headline MAE (and optionally WAPE), with RMSE and MAPE as supporting evidence.

### ğŸ–¼ï¸ Visualizations
- Actual vs Predicted (parity plot) with optional smearing lines (before/after), LOWESS trend, and metric box.
- MAE by segment (Sex / Region / Children) â†’ flags parity gaps.
- Correlation heatmap (incl. log_charges) for quick EDA.
- All plotting snippets are ready to paste. Colors use accessible palettes (Okabe-Ito, Tableau10, Navy).

### ğŸ” Sensitivity analysis
â€œWhat moves predictions?â€ Compute % change in predicted dollars for small perturbations:
Numerics: +kÂ·Ïƒ (e.g., +0.1Ïƒ, +1Ïƒ) per feature.
Categoricals: baseline (mode) â†’ each alternative level.
Outputs:
- Row-wise effects and summary tables (median, 5thâ€“95th percentile).
- Aggregated mean |impact| by base feature â†’ ranked drivers.
- Drop-in cell provided (uses your fitted pipeline + smearing). No refits required.

### âš–ï¸ Fairness slices & practical recommendations
Slices used: Sex / Region / Children
Compare MAE per group for both RF and XGB.
If Region shows the largest gap, collect:
 - Facility/provider ID, network tier, clinic type/DRG, negotiated unit prices, urban/rural flag.

Utilization proxies: visit counts, chronic flags, medication classes â†’ narrows variance for high-charge tails.

Policy granularity: deductible, co-pay, plan tier â†’ aligns premiums with benefit design.

### ğŸ” Reproducibility & seeds
- Set random_state across split, models, and tuning.
- Boosting can be seed-sensitive (train/test splits change residuals, and BayesSearchCV samples different configs).
- To assess robustness: evaluate across a small seed sweep (e.g., 42) and report variability.
- If performance swings a lot with seeds, consider: larger CV folds, more data, tighter search space, or stronger regularization.

### ğŸ› ï¸ Common errors & fixes

1. ImportError: cannot import name BayesSearchCV from sklearn.model_selection Use from skopt import BayesSearchCV (itâ€™s in scikit-optimize, not sklearn).
2. Import 'skopt.space' could not be resolved pip install -U scikit-optimize
3. XGBoost categorical error If using a pandas DataFrame with object dtypes directly in xgboost.DMatrix, youâ€™ll get Invalid columns: ... object. Fix: OHE in a ColumnTransformer (as in this repo), or enable_categorical=True with pandas Categorical.
4. Duan smearing makes MAE worse Smearing corrects mean bias on the dollar scale; it can reduce RMSE but sometimes increase MAE. Keep both reported; choose the KPI that matches business priorities.
5. Pillow _imaging arch mismatch on Apple Silicon Reinstall for arm64 wheel: pip uninstall pillow && pip install --no-binary=:all: pillow or use pip install --upgrade --force-reinstall pillow in a arm64 Python.
6. Git pull overwrote local work Use feature branches; when in doubt: git stash â†’ git pull â†’ resolve â†’ git stash pop. For PRs: push your branch, open compare main....


